{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap DQN\n",
    "### Function approximation Q-learning\n",
    "This tutorial walks through the implementation of Bootstrap deep Q networks (Bootstrap-DQNs), \n",
    "an RL method which applies the function approximation capabilities of deep neural networks\n",
    "to problems in reinforcement learning.\n",
    "The model in this tutorial closely follows the work described in the paper \n",
    "[Deep Exploration via Bootstrapped DQN](https://arxiv.org/abs/1602.04621), written by Ian Osband. \n",
    "\n",
    "To keep these chapters runnable \n",
    "by as many people as possible, \n",
    "on as many machines as possible,\n",
    "and with as few headaches as possible, \n",
    "we have so far avoided dependencies on external libraries \n",
    "(besides mxnet, numpy and matplotlib). \n",
    "However, in this case, we'll need to import the [OpenAI Gym](https://gym.openai.com/docs).\n",
    "That's because in reinforcement learning, \n",
    "instead of drawing examples from a data structure, \n",
    "our data comes from interactions with an environment. \n",
    "In this chapter, our environemnts will be classic Atari video games.\n",
    "\n",
    "## Preliminaries\n",
    "The following code clones and installs the OpenAI gym.\n",
    "`git clone https://github.com/openai/gym ; cd gym ; pip install -e .[all]` \n",
    "Full documentation for the gym can be found on [at this website](https://gym.openai.com/).\n",
    "If you want to see reasonable results before the sun sets on your AI career,\n",
    "we suggest running these experiments on a server equipped with GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import gym\n",
    "import math\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import pickle\n",
    "import logging, logging.handlers\n",
    "\n",
    "gpu_n = 0\n",
    "\n",
    "command = 'mkdir data'\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the algorithm\n",
    "#### Collect samples\n",
    "At the beginning of each episode (one round of the game), \n",
    "reset the environment to its initial state using `env.reset()`. \n",
    "At each time step ``t``, the environment is at `current_state`.\n",
    "With probability $\\epsilon$, apply a random action.\n",
    "Otherwise, choose on of the heads uniformly at random $i\\sim rand(1,10)$ and play according that head, i.e. apply $argmax_a~ Q_i(\\phi($ `current_state` $),a,\\theta)$,\n",
    "where $Q_i$ is parameterized by paramters $\\theta_i$ and $\\phi(\\cdot)$ is preprocessor.\n",
    "Pass the action through `env.step(action)` to receive next frame, reward and whether the game terminates.\n",
    "Append this frame to the end of the `current_state` and construct `next_state` while removeing $frame(t-12)$.\n",
    "Store the tuple $(\\phi($ `current_state` $), action, reward, \\phi($ `next_ state` $))$ in the replay buffer.\n",
    "\n",
    "#### Update Network\n",
    "* Draw batches of tuples from the replay buffer: $(\\phi,r,a,\\phi')$.\n",
    "* Define the following loss for each head i\n",
    "$$\\Large(\\small Q_i(\\phi,a,\\theta)-r-Q_i(\\phi',argmax_{a'}Q_i(\\phi',a',\\theta),\\theta^-)\\Large)^2$$\n",
    "* Where $\\theta^-$ is the parameter of the target network.( Set $Q_i(\\phi',a',\\theta^-)$ to zero if $\\phi$ is the preprocessed termination state). \n",
    "* Update the $\\theta$\n",
    "* Update the $\\theta^-$ once in a while\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        #Articheture\n",
    "        self.batch_size = 32 # The size of the batch to learn the Q-function\n",
    "        self.image_size = 84 # Resize the raw input frame to square frame of size 80 by 80 \n",
    "        #Trickes\n",
    "        self.replay_buffer_size = 1000000 # The size of replay buffer; set it to size of your memory (.5M for 50G available memory)\n",
    "        self.learning_frequency = 4 # With Freq of 1/4 step update the Q-network\n",
    "        self.skip_frame = 4 # Skip 4-1 raw frames between steps\n",
    "        self.internal_skip_frame = 4 # Skip 4-1 raw frames between skipped frames\n",
    "        self.frame_len = 4 # Each state is formed as a concatination 4 step frames [f(t-12),f(t-8),f(t-4),f(t)]\n",
    "        self.Target_update = 10000 # Update the target network each 10000 steps\n",
    "        self.epsilon_min = 0.01 # Minimum level of stochasticity of policy (epsilon)-greedy\n",
    "        self.annealing_end = 1000000. # The number of step it take to linearly anneal the epsilon to it min value\n",
    "        self.gamma = 0.99 # The discount factor\n",
    "        self.replay_start_size = 50000 # Start to backpropagated through the network, learning starts\n",
    "        self.no_op_max = 30 / self.skip_frame # Run uniform policy for first 30 times step of the beginning of the game\n",
    "        self.K = 10 #number of DQN\n",
    "        #otimization\n",
    "        self.num_episode = 100000 # Number episode to run the algorithm\n",
    "        self.max_frame = 10000000\n",
    "        self.lr = 0.00025 # RMSprop learning rate\n",
    "        self.gamma1 = 0.95 # RMSprop gamma1\n",
    "        self.gamma2 = 0.95 # RMSprop gamma2\n",
    "        self.rms_eps = 0.01 # RMSprop epsilon bias\n",
    "        self.ctx = mx.gpu(gpu_n) # Enables gpu if available, if not, set it to mx.cpu()\n",
    "opt = Options()\n",
    "\n",
    "env_name = 'AmidarNoFrameskip-v4' # Set the desired environment\n",
    "env = gym.make(env_name)\n",
    "num_action = env.action_space.n # Extract the number of available action from the environment setting\n",
    "\n",
    "logger = logging.getLogger()\n",
    "f_name = './data/results_Boots_%s_lr_%f.log' %(env_name,opt.lr)\n",
    "fh = logging.handlers.RotatingFileHandler(f_name)\n",
    "fh.setLevel(logging.DEBUG)#no matter what level I set here\n",
    "formatter = logging.Formatter('%(asctime)s:%(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "manualSeed = 1 # random.randint(1, 10000) # Set the desired seed to reproduce the results\n",
    "mx.random.seed(manualSeed)\n",
    "attrs = vars(opt)\n",
    "ff =(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "logging.error(str(ff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DQN model\n",
    "The network is constructed as three CNN layers and a fully connected added on the top. Furthermore, the optimizer is assigned to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DQN = gluon.nn.Sequential()\n",
    "with DQN.name_scope():\n",
    "    #first layer\n",
    "    DQN.add(gluon.nn.Conv2D(channels=32, kernel_size=8,strides = 4,padding = 0))\n",
    "    DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    DQN.add(gluon.nn.Activation('relu'))\n",
    "    #second layer\n",
    "    DQN.add(gluon.nn.Conv2D(channels=64, kernel_size=4,strides = 2))\n",
    "    DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    DQN.add(gluon.nn.Activation('relu'))\n",
    "    #tird layer\n",
    "    DQN.add(gluon.nn.Conv2D(channels=64, kernel_size=3,strides = 1))\n",
    "    DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    DQN.add(gluon.nn.Activation('relu'))\n",
    "    DQN.add(gluon.nn.Flatten())\n",
    "    #fourth layer\n",
    "def HEAD():\n",
    "    head = gluon.nn.Sequential()\n",
    "    with head.name_scope():\n",
    "        #fourth layer\n",
    "        head.add(gluon.nn.Dense(512,activation ='relu'))\n",
    "        #fifth layer\n",
    "        head.add(gluon.nn.Dense(num_action,activation ='relu'))\n",
    "    head.collect_params().initialize(mx.init.Normal(0.02), ctx=opt.ctx)\n",
    "    return head\n",
    "dqn = DQN\n",
    "dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=opt.ctx)\n",
    "DQN_trainer = gluon.Trainer(dqn.collect_params(),'RMSProp', \\\n",
    "                          {'learning_rate': opt.lr ,'gamma1':opt.gamma1,'gamma2': opt.gamma2,'epsilon': opt.rms_eps,'centered' : True})\n",
    "\n",
    "\n",
    "heads = gluon.nn.Sequential()\n",
    "for _ in range(opt.K):\n",
    "    heads.add(HEAD())\n",
    "\n",
    "\n",
    "heads_trainer = []\n",
    "\n",
    "for i in range(opt.K):\n",
    "    heads_trainer.append(gluon.Trainer(heads[i].collect_params(),'RMSProp', \\\n",
    "                          {'learning_rate': opt.lr ,'gamma1':opt.gamma1,'gamma2': opt.gamma2,'epsilon': opt.rms_eps,'centered' : True}))\n",
    "\n",
    "\n",
    "dqn.collect_params().zero_grad()\n",
    "\n",
    "for i in range(opt.K):\n",
    "    heads[i].collect_params().zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Target_DQN = gluon.nn.Sequential()\n",
    "with Target_DQN.name_scope():\n",
    "    #first layer\n",
    "    Target_DQN.add(gluon.nn.Conv2D(channels=32, kernel_size=8,strides = 4,padding = 0))\n",
    "    Target_DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    Target_DQN.add(gluon.nn.Activation('relu'))\n",
    "    #second layer\n",
    "    Target_DQN.add(gluon.nn.Conv2D(channels=64, kernel_size=4,strides = 2))\n",
    "    Target_DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    Target_DQN.add(gluon.nn.Activation('relu'))\n",
    "    #tird layer\n",
    "    Target_DQN.add(gluon.nn.Conv2D(channels=64, kernel_size=3,strides = 1))\n",
    "    Target_DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    Target_DQN.add(gluon.nn.Activation('relu'))\n",
    "    Target_DQN.add(gluon.nn.Flatten())\n",
    "\n",
    "target_dqn = Target_DQN\n",
    "target_dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=opt.ctx)\n",
    "\n",
    "heads_target = gluon.nn.Sequential()\n",
    "for _ in range(opt.K):\n",
    "    heads_target.add(HEAD())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer\n",
    "Replay buffer store the tuple of : `state`, action , `next_state`, reward , done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward','done'))\n",
    "class Replay_Buffer():\n",
    "    def __init__(self, replay_buffer_size):\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.replay_buffer_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.replay_buffer_size\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess frames\n",
    "* Take a frame, average over the `RGB` filter and append it to the `state` to construct `next_state`\n",
    "* Clip the reward\n",
    "* Render the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(raw_frame, currentState = None, initial_state = False):\n",
    "    raw_frame = nd.array(raw_frame,mx.cpu())\n",
    "    raw_frame = nd.reshape(nd.mean(raw_frame, axis = 2),shape = (raw_frame.shape[0],raw_frame.shape[1],1))\n",
    "    raw_frame = mx.image.imresize(raw_frame,  opt.image_size, opt.image_size)\n",
    "    raw_frame = nd.transpose(raw_frame, (2,0,1))\n",
    "    raw_frame = raw_frame.astype(np.float32)/255.\n",
    "    if initial_state == True:\n",
    "        state = raw_frame\n",
    "        for _ in range(opt.frame_len-1):\n",
    "            state = nd.concat(state , raw_frame, dim = 0)\n",
    "    else:\n",
    "        state = mx.nd.concat(currentState[1:,:,:], raw_frame, dim = 0)\n",
    "    return state\n",
    "\n",
    "def rew_clipper(rew):\n",
    "    if rew>0.:\n",
    "        return 1.\n",
    "    elif rew<0.:\n",
    "        return -1.\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def renderimage(next_frame):\n",
    "    if render_image:\n",
    "        plt.imshow(next_frame);\n",
    "        plt.show()\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(.1)\n",
    "        \n",
    "l2loss = gluon.loss.L2Loss(batch_axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_frame = env.reset()\n",
    "state = preprocess(next_frame, initial_state = True)\n",
    "data = nd.array(state.reshape([1,opt.frame_len,opt.image_size,opt.image_size]),opt.ctx)\n",
    "dqn(data).shape\n",
    "target_dqn(data).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame_counter = 0. # Counts the number of steps so far\n",
    "annealing_count = 0. # Counts the number of annealing steps\n",
    "epis_count = 0. # Counts the number episodes so far\n",
    "replay_memory = Replay_Buffer(opt.replay_buffer_size) # Initialize the replay buffer\n",
    "tot_clipped_reward = []\n",
    "tot_reward = []\n",
    "frame_count_record = []\n",
    "moving_average_clipped = 0.\n",
    "moving_average = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "render_image = False # Whether to render Frames and show the game\n",
    "batch_state = nd.empty((opt.batch_size,opt.frame_len,opt.image_size,opt.image_size), opt.ctx)\n",
    "batch_state_next = nd.empty((opt.batch_size,opt.frame_len,opt.image_size,opt.image_size), opt.ctx)\n",
    "while epis_count < opt.max_frame:\n",
    "    cum_clipped_reward = 0\n",
    "    cum_reward = 0\n",
    "    next_frame = env.reset()\n",
    "    state = preprocess(next_frame, initial_state = True)\n",
    "    t = 0.\n",
    "    done = False\n",
    "    k = random.randint(0,opt.K-1)\n",
    "    mx.nd.waitall()\n",
    "\n",
    "    while not done:\n",
    "        previous_state = state\n",
    "        # show the frame\n",
    "        renderimage(next_frame)\n",
    "        sample = random.random()\n",
    "        if frame_counter > opt.replay_start_size:\n",
    "            annealing_count += 1\n",
    "        if frame_counter == opt.replay_start_size:\n",
    "            logging.error('annealing and laerning are started ')\n",
    "            \n",
    "            \n",
    "        \n",
    "        eps = np.maximum(1.-annealing_count/opt.annealing_end,opt.epsilon_min)\n",
    "        effective_eps = eps\n",
    "        if t < opt.no_op_max:\n",
    "            effective_eps = 1.\n",
    "        \n",
    "        # epsilon greedy policy\n",
    "        if sample < effective_eps:\n",
    "            action = random.randint(0, num_action - 1)\n",
    "        else:\n",
    "            data = nd.array(state.reshape([1,opt.frame_len,opt.image_size,opt.image_size]),opt.ctx)\n",
    "            action = int(nd.argmax(heads[k](dqn(data)),axis=1).as_in_context(mx.cpu()).asscalar())\n",
    "        \n",
    "        # Skip frame\n",
    "        rew = 0\n",
    "        for skip in range(opt.skip_frame-1):\n",
    "            next_frame, reward, done,_ = env.step(action)\n",
    "            renderimage(next_frame)\n",
    "            cum_clipped_reward += rew_clipper(reward)\n",
    "            rew += reward\n",
    "            for internal_skip in range(opt.internal_skip_frame-1):\n",
    "                _ , reward, done,_ = env.step(action)\n",
    "                cum_clipped_reward += rew_clipper(reward)\n",
    "                rew += reward\n",
    "                \n",
    "        next_frame_new, reward, done, _ = env.step(action)\n",
    "        renderimage(next_frame)\n",
    "        cum_clipped_reward += rew_clipper(reward)\n",
    "        rew += reward\n",
    "        cum_reward += rew\n",
    "        \n",
    "        # Reward clipping\n",
    "        reward = rew_clipper(rew)\n",
    "        next_frame = np.maximum(next_frame_new,next_frame)\n",
    "        state = preprocess(next_frame, state)\n",
    "        replay_memory.push(previous_state,action,state,reward,done)\n",
    "        # Train\n",
    "        if frame_counter > opt.replay_start_size:        \n",
    "            if frame_counter % opt.learning_frequency == 0:\n",
    "                transitions = replay_memory.sample(opt.batch_size)\n",
    "                batch = Transition(*zip(*transitions))\n",
    "                for j in range(opt.batch_size):\n",
    "                    batch_state[j] = nd.array(batch.state[j],opt.ctx)\n",
    "                    batch_state_next[j] = nd.array(batch.next_state[j],opt.ctx)\n",
    "                batch_reward = nd.array(batch.reward,opt.ctx)\n",
    "                batch_action = nd.array(batch.action,opt.ctx).astype('int32')\n",
    "                batch_done = nd.array(batch.done,opt.ctx)\n",
    "\n",
    "                with autograd.record():\n",
    "                    output = dqn(batch_state_next)\n",
    "                    output_target = target_dqn(batch_state_next)\n",
    "                    loss = 0\n",
    "                    for h in range(opt.K):\n",
    "                        argmax_Q = nd.argmax(heads[h](output),axis = 1)#.astype('int32')\n",
    "                        Q_sp = nd.pick(heads_target[h](output_target),argmax_Q,1)\n",
    "                        Q_sp = Q_sp*(nd.ones(opt.batch_size,ctx = opt.ctx)-batch_done)\n",
    "                        Q_s_array = dqn(batch_state)\n",
    "                        Q_s = nd.pick(Q_s_array,batch_action,1)\n",
    "                        loss = loss + nd.mean(l2loss(Q_s ,  (batch_reward + opt.gamma *Q_sp)))/opt.K                    \n",
    "                loss.backward()\n",
    "                DQN_trainer.step(opt.batch_size)\n",
    "                for h in range(opt.K):\n",
    "                    heads_trainer[h].step(opt.batch_size)\n",
    "        \n",
    "        t += 1\n",
    "        frame_counter += 1\n",
    "        \n",
    "        # Save the model and update Target model\n",
    "        if frame_counter > opt.replay_start_size:\n",
    "            if frame_counter % opt.Target_update == 0 :\n",
    "                check_point = frame_counter / (opt.Target_update *100)\n",
    "                fdqn = './data/Boots_target_%s_%d' % (env_name,int(check_point))\n",
    "                dqn.save_params(fdqn)\n",
    "                target_dqn.load_params(fdqn, opt.ctx)\n",
    "                for h in range(opt.K):\n",
    "                    fdqn = './data/Boots_target_head_%s_%d_%d' % (env_name,int(check_point),h)\n",
    "                    heads[h].save_params(fdqn)\n",
    "                    heads_target[h].load_params(fdqn, opt.ctx)                    \n",
    "                fnam = './data/Boots_clippted_rew%s_lr_%d' %(env_name,opt.lr)\n",
    "                np.save(fnam,tot_clipped_reward)\n",
    "                fnam = './data/Boots_tot_rew%s_lr_%f' %(env_name,opt.lr)\n",
    "                np.save(fnam,tot_reward)\n",
    "                fnam = './data/Boots_frame_count%s_lr_%f' %(env_name,opt.lr)\n",
    "                np.save(fnam,frame_count_record) \n",
    "                \n",
    "        if done:\n",
    "            if epis_count % 100. == 0. :\n",
    "                logging.error('Boots-env:%s,gpu:[%d],lr:%d,epis[%d],eps[%f],durat[%d],fnum=%d, cum_cl_rew = %d, cum_rew = %d,tot_cl = %d , tot = %d'\\\n",
    "                  %(env_name,gpu_n,opt.lr,epis_count,eps,t+1,frame_counter,cum_clipped_reward,cum_reward,moving_average_clipped,moving_average))\n",
    "    epis_count += 1\n",
    "    tot_clipped_reward = np.append(tot_clipped_reward, cum_clipped_reward)\n",
    "    tot_reward = np.append(tot_reward, cum_reward)\n",
    "    frame_count_record = np.append(frame_count_record,frame_counter)\n",
    "    if epis_count > 100.:\n",
    "        moving_average_clipped = np.mean(tot_clipped_reward[int(epis_count)-1-100:int(epis_count)-1])\n",
    "        moving_average = np.mean(tot_reward[int(epis_count)-1-100:int(epis_count)-1])\n",
    "from tempfile import TemporaryFile\n",
    "outfile = TemporaryFile()\n",
    "outfile_clip = TemporaryFile()\n",
    "np.save(outfile, moving_average)\n",
    "np.save(outfile_clip, moving_average_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the overall performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bandwidth = 1000 # Moving average bandwidth\n",
    "total_clipped = np.zeros(int(epis_count)-bandwidth)\n",
    "total_rew = np.zeros(int(epis_count)-bandwidth)\n",
    "for i in range(int(epis_count)-bandwidth):\n",
    "    total_clipped[i] = np.sum(tot_clipped_reward[i:i+bandwidth])/bandwidth\n",
    "    total_rew[i] = np.sum(tot_reward[i:i+bandwidth])/bandwidth\n",
    "t = np.arange(int(epis_count)-bandwidth)\n",
    "belplt = plt.plot(t,total_rew[0:int(epis_count)-bandwidth],\"r\", label = \"Return\")\n",
    "plt.legend()#handles[likplt,belplt])\n",
    "print('Running after %d number of episodes' %epis_count)\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average Reward per episode\")\n",
    "plt.show()\n",
    "likplt = plt.plot(t,total_clipped[0:opt.num_episode-bandwidth],\"b\", label = \"Clipped Return\")\n",
    "plt.legend()#handles[likplt,belplt])\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average clipped Reward per episode\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
